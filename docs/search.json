[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelagem de Preço de Venda de Casas",
    "section": "",
    "text": "1 Introdução\nA Estatística pode ser aplicada em diversas áreas. No cenário atual onde análise de dados se tornou um ponto crucial na tomada de decisão em meio a todo tipo de negócio, a análise do comércio imobiliário não é diferente. Tendo os resultados aqui apresentados, com a finalidade de ajudar na tomada de decisão de compra, venda e investimento. A determinação precisa dos fatores que afetam os preços das propriedades pode beneficiar compradores, vendedores, corretores e investidores ao fornecer informações críticas para tomada de decisão. Este estudo foca na previsão do preço de venda de casas no Condado de King, Seattle, utilizando técnicas de regressão linear múltipla. O objetivo deste trabalho é identificar a relação entre o preço de venda de casas e suas características, na construção e diagnóstico dos modelos preditivos.\nA análise será realizada usando uma amostra do conjunto de dados, que abrange vendas de casas entre maio de 2014 e maio de 2015, disponibilizado pela plataforma denominada de Kaggle. Será falado mais sobre o conjunto de dados.\nO modelo obtido será avaliado por meio de testes de significância estatística, verificação dos pontos de influência, homocedasticidade, normalidade dos resíduos e entre outros técnicas e medidas.\nEm resumo, este estudo pretende contribuir para uma compreensão mais profunda dos fatores que influenciam os preços das casas em Seattle, oferecendo uma análise rigorosa e comparativa que pode ser aplicada em contextos semelhantes no mercado imobiliário e em outros campos de pesquisa."
  },
  {
    "objectID": "intro.html#conjunto-de-dados",
    "href": "intro.html#conjunto-de-dados",
    "title": "2  Material e Métodos",
    "section": "2.1 Conjunto de Dados",
    "text": "2.1 Conjunto de Dados\nComo já dito, o conjunto de dados utilizado neste estudo foi obtido do . Este conjunto contém informações sobre vendas de casas no Condado de King, Seattle, entre maio de 2014 e maio de 2015, totalizando 21.613 observações e 21 variáveis. Porém, foi retirada uma amostra de tal conjunto. Visto que o princípio da Inferência Estatística é generalizar, concluir ou inferir os resultados de um conjunto pequeno para um conjunto maior, se tornado mais adequado para esse caso, visando um menor custo.\nFoi retirada uma amostra aleatória simples, que de forma simples e operante, parte de uma lista com \\(N\\) unidades elementares, realiza-se um sorteio, com probabilidades iguais, de \\(n\\) unidades . Utilizando uma fórmula mais simples para se calcular o tamanho da amostra, \\[n = \\dfrac{N \\times n_0}{N + n_0},\\] onde \\(n_0 = \\frac{1}{\\varepsilon^2}\\), sendo \\(\\varepsilon\\) a margem de erro amostral admitido. Fixando um \\(\\varepsilon = 0,03\\), isto é, admitindo um erro amostral de 3%, \\(n = 1.057\\). No Quadro (\\(\\ref{quadro:01}\\)), são apresentadas as variáveis preditoras e suas descrições.\nSendo a variável resposta, as demais variáveis fornecem uma visão abrangente das características das casas e são fundamentais para o desenvolvimento e análise dos modelos preditivos.\nFoi realizada uma manipulação no conjunto de dados para torná-lo mais adequado à aplicação de regressão linear múltipla. Da variável preditora , foram extraídas informações adicionais e criadas quatro novas variáveis, conforme descrito no Quadro (\\(\\ref{quadro:02}\\))."
  },
  {
    "objectID": "intro.html#software",
    "href": "intro.html#software",
    "title": "2  Material e Métodos",
    "section": "2.2 Software",
    "text": "2.2 Software\nPara conduzir as análises e estimativas neste estudo, foi utilizada a linguagem de programação Python, empregando a IDE Google Colaboratory*. As seguintes bibliotecas foram utilizadas nas diversas etapas do processo:\n\nNumPy: Para operações matemáticas, lógicas e estatísticas eficientes em vetores multidimensionais ou matrizes.\nPandas: Para manipulação e análise de dados, oferecendo estruturas de dados flexíveis e poderosas.\nMatplotlib: Para criação de visualizações gráficas.\nSeaborn: Complementar ao Matplotlib, oferece uma interface de alto nível para criação de gráficos estatísticos atrativos e informativos.\nScipy: Para cálculos científicos e técnicos.\nStatsmodels: Para estimar e inferir modelos estatísticos"
  },
  {
    "objectID": "intro.html#metodologia",
    "href": "intro.html#metodologia",
    "title": "2  Material e Métodos",
    "section": "2.3 Metodologia",
    "text": "2.3 Metodologia\nO processo de análise e modelagem dos dados para prever o preço das casas, deu-se de forma:\n\nTestes de Normalidade e Transformações: Um dos pressupostos para aplicação de regressão linear é de que a variável resposta, \\(Y\\), siga uma distribuição normal. Os testes de normalidade que foram aplicados nesse trabalho foram: Shapiro-Wilk, Kolmogorov-Smirnov, D’Agostino-Pearson e Jarque-Bera.\n\nCaso a variável resposta \\(Y\\) não siga uma distribuição gaussiana, será aplicado a Transformação Box-Cox.\n\nAnálise Exploratória de Dados (AED): Realização de análises estatísticas por meio de medidas de resumo e visualizações gráficas para entender a distribuição e variabilidade dos dados, identificar padrões e relações nos dados.\n\nMedidas Descritivas: são fundamentais para uma análise exploratória de dados (AED) robusta e informativa. Elas oferecem um resumo estatístico dos dados, possibilitando uma compreensão rápida das principais características das variáveis preditoras e da variável resposta. Incluem média, desvio padrão, valores mínimos, máximos e quartis (25%, mediana e 75%).\nAnálise Gráfica: crucial na AED para visualizar intuitivamente a distribuição dos dados e as relações entre variáveis. Foram utilizados os tipos de gráficos abaixo, com objetivos específicos:\n\nHistogramas: Visualizam a distribuição das variáveis numéricas, destacando a frequência dos valores e padrões como assimetrias e outliers através de curvas de densidade (KDE).\nBoxplots: Mostram a dispersão, assimetria e outliers das variáveis, sendo úteis para comparações entre grupos.\nDiagramas de Dispersão: Exploram relações entre variáveis numéricas, identificando padrões e correlações.\nMatriz de Correlação: Mostra relações lineares entre todas as variáveis numéricas, ajudando na detecção de multicolinearidade e na seleção de variáveis para modelos de regressão.\n\n\nModelagem Estatística: Ajuste de modelos de regressão linear múltipla para prever o preço das casas.\nDiagnóstico e Análise de Resíduos: Avaliação dos modelos por meio de análise de resíduos, medidas para pontos aberrantes e influentes como alavancagem, Distância de Cook e DFFITS. Verificação de pressupostos de normalidade e homocedasticidade.\n\nPara todos os Teste de Hipóteses e Intervalos de Confiança foi fixado um nível de significância de 3%, assim, \\(\\alpha = 0,03\\)."
  },
  {
    "objectID": "intro.html#conceitos-e-definições",
    "href": "intro.html#conceitos-e-definições",
    "title": "2  Material e Métodos",
    "section": "2.4 Conceitos e Definições",
    "text": "2.4 Conceitos e Definições\n\n2.4.1 Transformação Box-Cox\nSugerida em 1964 por George E. P. Box e David R. Cox, que propuseram a família de transformações de potência (não-lineares) expressa pela Equação (\\(\\ref{equation:02}\\)).\n\\[\n\\begin{equation}\ny^{(\\lambda)} = \\left\\{ \\begin{array}{l}\n\\dfrac{y^\\lambda - 1}{\\lambda} \\text{, se } \\lambda \\neq 0 \\text{, } y > 0 \\\\\n\\log  y \\text{, se } \\lambda = 0 \\text{, } y > 0\n\\end{array} \\right.\n\\label{equation:02}\n\\end{equation}\n\\]\nO valor do parâmetro \\(\\lambda\\), pode ser estimado pelo método de Máxima Verossimilhança. O princípio da verossimilhança afirma que devemos escolher o valor do parâmetro desconhecido, com base em uma amostra aleatória, que maximize a probabilidade desta amostra aleatória em questão ser obtida, ou seja, o valor que torna tal amostra a “mais provável” .\nAssumindo que, as observações transformadas \\(y_i^\\lambda\\), \\(i = 1, ..., n\\), satisfazem as suposições de uma distribuição gaussiana, ou seja, são i.i.d de forma normal com variância constante, \\(\\sigma^2\\), e esperança dada por, \\(E[Y_i^{(\\lambda)}] = \\boldsymbol{\\beta}\\mathbf{X_i}\\). Assim, para um \\(\\lambda\\) fixo, a função é, com exceção de constantes, dada na Equação (\\(\\ref{equation:03}\\)).\n\\[\n\\begin{equation}\n\\boldsymbol{l}(\\lambda) = - \\dfrac{n}{2} \\log \\hat{\\sigma^2}(\\lambda) + (\\lambda - 1) \\sum_{i=1}^n \\log y_i\n\\label{equation:03}\n\\end{equation}\n\\]\nOnde \\(\\hat{\\sigma^2}(\\lambda)\\) é obtido a partir da de \\(Y^{(\\lambda)}\\), sendo a soma de quadrados dos resíduos (\\(SQR\\)) divido pelos graus de liberdade (\\(gl\\)) do resíduo. Falaremos mais sobre isso à frente.\nEm resumo, um \\(\\lambda\\) que maximize \\(\\boldsymbol{l}(\\lambda)\\) é a melhor estimativa para o parâmetro da transformação. Para mais informações, se indica ver o brilhante trabalho de Box e Cox (1964) , e um estudo sobre as Transformações de Box-Cox, elaborado em uma Monografia do Departamento de Estatística da Universidade de Brasília .\n\n\n\n\n\n\n\n2.4.2 Modelo de Regressão Múltipla\nSeja \\(Y\\) a variável dependente (resposta) que desejamos estimar usando \\(p - 1\\) variáveis independentes (preditoras). Assim, cada observação \\(Y_i\\) é expressa como uma combinação linear das variáveis independentes \\(X_{ij}\\) acrescida de um termo estocástico \\(\\varepsilon_i\\), onde \\(j = 1, 2, \\ldots, p - 1\\), sendo \\(p\\) o número total de coeficientes de regressão, e \\(i = 1, 2, \\ldots, n\\), onde \\(n\\) é o tamanho da amostra. Veja a Equação (\\(\\ref{equation:04}\\)).\n\\[\n\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_{p - 1} X_{ip - 1} + \\varepsilon_i\n\\label{equation:04}\n\\end{equation}\n\\]\nA Equação (\\(\\ref{equation:04}\\)) pode ser expressa de forma matricial, como mostrado na Equação (\\(\\ref{equation:05}\\)).\n\\[\n\\begin{equation}\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\label{equation:05}\n\\end{equation}\n\\]\nOnde\n$$\n\\[\\begin{eqnarray}\n\\mathbf{Y_{n \\times 1}} = \\begin{bmatrix}\nY_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix},\n\\mathbf{X_{n \\times p}} = \\begin{bmatrix}\n1 & X_{11} & \\cdots & X_{1 \\text{ } p - 1} \\\\\n1 & X_{21} & \\cdots & X_{2 \\text{ } p - 1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & X_{n1} & \\cdots & X_{n \\text{ } p - 1}\n\\end{bmatrix},\n\\boldsymbol{\\beta_\\mathbf{p \\times 1}} = \\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p - 1}\n\\end{bmatrix},\n\\boldsymbol{\\varepsilon_\\mathbf{n \\times 1}} = \\begin{bmatrix}\n\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\n\\end{bmatrix}.\n\\end{eqnarray}\\] $$\n\\(\\mathbf{Y}\\) é o vetor de valores da variável resposta, \\(\\mathbf{X}\\) é a matriz de variáveis preditoras, \\(\\boldsymbol{\\beta}\\) vetor de coeficientes da regressão e \\(\\boldsymbol{\\varepsilon}\\) é componente estocástica. A regressão pelo método de mínimos quadrados tem como principio, estimar os coeficientes do modelo minimizando a soma dos quadrados dos resíduos, desta forma \\[{\\boldsymbol{\\hat\\beta}} = \\mathbf{( X^T X )^{-1} X^T Y}.\\] Quando os pressupostos da regressão pelo método de mínimos quadrados são atendidos, os coeficientes estimados são não viesados e possuem variância mínima dentre a classe de estimadores não viesados para \\(\\boldsymbol{\\beta}\\) .\n\n2.4.2.1 Variância em um Modelo de Regressão Múltipla\nNa análise de regressão múltipla, a variância dos erros (\\(\\sigma^2\\)) é um parâmetro fundamental para compreender a dispersão dos valores da variável dependente \\(Y\\) em relação ao modelo ajustado, que incorpora múltiplas variáveis independentes. A estimativa dessa variância é essencial para realizar inferências estatísticas, como a construção de intervalos de confiança e a realização de testes de hipóteses.\nJá foi mostrado nesse trabalho que em um modelo de regressão múltipla, cada observação \\(Y_i\\) é expressa como uma combinação linear de variáveis independentes \\(X_{ij}\\) somada de um termo estocástico \\(\\varepsilon_i\\), onde \\(j = 1, 2, \\ldots, p - 1\\) e \\(i = 1, 2, \\ldots, n\\), expressa pela Equação (\\(\\ref{equation:04}\\)).\nOs erros ou resíduos \\(\\varepsilon_i\\) são as diferenças entre os valores observados \\(Y_i\\) e os valores preditos \\(\\hat{Y}_i\\) pelo modelo ajustado, logo,\n\\[\n\\varepsilon_i = Y_i - \\hat{Y}_i.\n\\]\nEm regressão múltipla, o número de graus de liberdade dos resíduos é \\(n - p\\), onde \\(p\\) é o número de parâmetros a serem estimados e \\(n\\) é o tamanho da amostra. Assim, a soma dos quadrados dos resíduos é dividida pelos graus de liberdade para obter o quadrado médio do erro (QME), desta forma\n\\[\nQME = \\frac{SQE}{n - p},\n\\]\nsendo o QME um estimador não tendencioso para a variância dos erros, sendo está igual a variância populacional de \\(Y\\), logo, \\(E[QME] = \\sigma^2\\). Essa estimativa é crucial para diversas análises no contexto de regressão múltipla. Um menor \\(\\hat{\\sigma^2}\\) indica que os valores de \\(Y\\) estão mais próximos dos valores preditos pelo modelo, sugerindo um melhor ajuste. Por outro lado, um maior \\(\\hat{\\sigma^2}\\) indica maior dispersão dos dados em torno da superfície de regressão, sugerindo que o modelo pode não capturar adequadamente a relação entre \\(Y\\) e as variáveis independentes \\(X_j\\).\nAlém disso, a estimativa de \\(\\sigma^2\\), ou seja, \\(\\hat{\\sigma^2}\\), é usada para calcular o erro padrão dos coeficientes de regressão, permitindo testar a significância desses coeficientes e construir intervalos de confiança. Também é essencial na análise de resíduos, proporcionando uma avaliação robusta do modelo. Para uma melhor compreensão de tal assunto, deixa-se as referências .\n\n\n\n\n\n\n\n\n2.4.3 Diagnóstico\n\n2.4.3.1 Pontos de Alavanca\nA matriz de projeção H,\n\\[\nH = \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T,\n\\]\né utilizada para identificar observações atípicas e influentes na amostra. É importante focar na diagonal principal dessa matriz, onde valores elevados de \\(h_{ii}\\) indicam observações que se destacam consideravelmente das demais, em relação ao vetor médio das variáveis preditoras.\nPara iniciar o diagnóstico, é crucial examinar os pontos de alavancagem, que são observações com \\(h_{ii}\\) superior ao limiar \\(\\frac{2p}{n}\\), portanto:\n\\[\nh_{ii} \\geq \\frac{2p}{n},\n\\]\nonde \\(p\\) é o número de parâmetros (coeficientes) da regressão e \\(n\\) é o tamanho da amostra. Esse método tende a detectar muitos pontos de alavancagem, o que justifica a utilização de técnicas adicionais de diagnóstico .\n\n\n2.4.3.2 Pontos de Influência\nDefine-se como ponto de influência a observação que, ao ser removida do conjunto de dados, causa mudanças significativas no vetor de coeficientes da regressão. As medidas para verificação de pontos de influência incluem:\n\nDistância de Cook: Avalia o quão atípica é uma observação, combinando o resíduo padronizado com os pontos de alavanca, conforme a Equação (\\(\\ref{equation:13}\\)).\n\n\\[\n\\begin{equation}\nD_i = \\dfrac{h_{ii}}{p(1 - h_{ii})} (\\varepsilon_i^*)^2\n\\label{equation:13}\n\\end{equation}\n\\]\nA Distância de Cook pode não ser adequada quando o resíduo padronizado é grande e \\(h_{ii}\\) é próximo de zero. Compara-se \\(D_i\\) com o valor crítico da distribuição \\(F-Snedecor\\) com \\(p\\) e \\(n - p\\) graus de liberdade, ao nível de significância \\(\\alpha = 50\\%\\), valor aproximadamente igual a 1 na prática. Assim, se \\(D_i \\geq 1\\), a i-ésima observação é considerada um ponto influente.\n\nDFFITS: Uma medida alternativa à Distância de Cook, obtida a partir do resíduo studentizado e da medida de alavancagem, conforme a Equação (\\(\\ref{equation:14}\\)).\n\n\\[\n\\begin{equation}\nDFFITS_i = t_i \\sqrt{ \\dfrac{h_{ii}}{p(1 - h_{ii})} }\n\\label{equation:14}\n\\end{equation}\n\\]\nSe \\(DFFITS_i \\geq 2 \\sqrt{ \\dfrac{p}{n - p} }\\), a i-ésima observação é considerada um ponto influente.\n\n\n2.4.3.3 Análise de Resíduos\nA análise de resíduos é essencial para diagnosticar a adequação do modelo de regressão linear à variável resposta \\(Y\\), assumindo que \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\)."
  }
]